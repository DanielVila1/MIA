{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3XecmVcvdAgu"
      },
      "source": [
        "# Authors:\n",
        "\n",
        "Daniel Vila de la Cruz\n",
        "\n",
        "Sara Gómez Feás"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg40l_UbdGFI"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bkxSSG_vfmov"
      },
      "source": [
        "For the VAEs , we divided the training process into multiple blocks, each consisting of a certain number of epochs. After training each block, we evaluated the performance of the VAE by calculating the FID, which measures the similarity between the original images and the reconstructed counterparts. Additionally, we visually compared the original and reconstructed images to gain qualitative insights into the model's progression."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rf9ij40yfmov"
      },
      "source": [
        "Due to limitations in computational resources and frequent session timeouts on Google Colab, we encountered challenges while training the VAE model with image sizes of 64 or higher. The interruptions in the training process required constant supervision, preventing simultaneous work on other tasks. In order to ensure the completion of the training process and overcome these obstacles, we made the decision to reduce the image size. Additionally, we considered the possibility of incorporating a more complex architecture by adding additional convolutional layers and/or by increasing the number of neurons. However, due to the limitations mentioned earlier, we were unable to fully execute and evaluate this approach."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "21KAs-iNfmow"
      },
      "source": [
        "In our initial approach, we adopted the architecture outlined in the lab notebooks for both the encoder and decoder components of the VAE model. Additionally, we set the latent dimension (z_dim) to 200. In subsequent iterations, we experimented with reducing the z_dim to 100 (third model) and also simplifying the architecture of both the encoder and decoder by removing one convolutional layer from each component (second model).\n",
        "\n",
        "The first model produced reconstructed images of human faces that were somewhat blurry but still recognizable. As the training progressed, the FID score improved, suggesting that the model learned to capture certain aspects of facial features. However, the fine details and sharpness were not well-represented. \n",
        "Similarly, the third model, which had a lower z_dim of 100 but the same architecture, also generated blurry reconstructed images of human faces. However, key facial features such as the eyes stood out prominently. This indicates that both models successfully learned to emphasize important facial characteristics during training. \n",
        "Comparing the two models, the reconstructed images from the first model appeared slightly better in quality than those from the third model, although the difference was not highly noticeable. \n",
        "In addition, the second model in the notebook initially produced poorly defined and smoothed reconstructed images. However, as the training progressed, there was an evident improvement in image quality, reflected by a lower FID score."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WtijHJzudGic"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LRWBHjH5dTIY"
      },
      "source": [
        "During the development of the GANs model we encountered several difficulties. The requirements for training a GAN model are high regarding time and computational resources, so our first approach was to reduce the size of the images to 32x32. But it wasn't enough, as the time requirements for training one single epoch were of more than 5 hours, so we were forced to reduce the number of epochs and the number of steps per epoch. We also adressed the dificulty of this architecture for tunning its hyperparameters as its hard to see how the model is performing on the first epochs, were images are too noisy, so this aggravates the time requirements, as we had to wait for the model to be fully trainned to check its performance and see if the parametter configuration is actually working. Furthermore, we had to deal with some problems related to Google Colab (as we needed it to use GPUs). Those problems were memory size limitations, GPUs not always availables, inability to compute cells with long time requirements and getting kicked out continuously during the training phase. To address those problems, we decided to, instead of repeating the data several times, make multiple calls to the fit function, reducing the memory allocated, the computing time of each cell and avoiding Colab to get stuck during long training periods.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VeTWAXNHlvzT"
      },
      "source": [
        "As a modification of the architecture, we decided to implement dilated convolutions, which consists on inserting holes between the kernel elements, allowing for an expansion of the receptive field without  increasing the number of parameters. However, this change resulted into more time required for training the model.\n",
        "\n",
        "The resulting images aren't that good, as they have a lot of noise, but most of them are identifiable as human faces. With a higher resolution the images would probably improve, at a cost of much more training time. Regarding the metrics, the critic presents a very high accuracy, as the generated images are easy to diferenciate from the real ones. Regarding the generator, it starts with a high accuracy and very bad image generations, and ends up with low accuracy and somehow decent images. So it reinforces the idea that the accuracy of a generator isn't a reliable source to test the goodness of the generated images. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kny4BZpdG5t"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uXKXFiJMvuXc"
      },
      "source": [
        "Comparing GANs and VAEs from our previous knowledge before implementing them, we saw that those models are theoretically more complex than the previous architectures we saw during this course. Also, they are hard to work with them, tunning its hyperparameters and looking for a better model is a tough task, being GAN models harder than VAEs. \n",
        "\n",
        "Regarding its results, both models were able to produce images that were identifiable as human faces, but were still fur away from looking like state-of-art models. When we studied VAEs we saw that they often produce blurry results, which was true. GANs were supposed to generate \"sharper\" results, which ended up being also true, but those images contain a lot of noisy pixels. It is worth noting that the comparison of results may not be entirely fair, because we just produced images on a gray scale for the VAE models, so pixels missplaced on color images will be more notorious."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
