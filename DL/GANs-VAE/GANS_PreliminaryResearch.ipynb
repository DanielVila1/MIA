{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Authors:\n",
        "\n",
        "Daniel Vila de la Cruz\n",
        "\n",
        "Sara Gómez Feás"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kGQHgmMtQjib"
      },
      "source": [
        "# Preliminary Research"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VF5scVlBQzAn"
      },
      "source": [
        "Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are two popular types of generative models. VAEs are known for creating well-structured latent spaces, where specific directions encode meaningful axes of variation in the data. On the other hand, GANs are capable of generating highly realistic images, but the latent spaces from which they originate are not as structured and continuous as those of VAEs."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JPBhUCwRK_pE"
      },
      "source": [
        "# Variational Autoencoders (VAEs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptllk9ztLbKu"
      },
      "source": [
        "Variational Autoencoders (VAEs) are generative models used to generate new data samples from the learned probability distribution in a latent space. They are also graphical models, as they use a graphical representation to model the relationship between observed and latent variables.\n",
        "\n",
        "The architecture can be represented as:\n",
        "\n",
        "1.   **Encoding**: The input data is passed through an encoder neural network, which maps the input variable to a latent space that corresponds to the parameters of a variational distribution. This latent space represents a compressed representation of the input data.\n",
        "2.   **Sampling**: A sample is drawn from the variational distribution in the latent space. This sample represents a possible value for the latent variable.\n",
        "3.   **Decoding**: The sample from the latent space is passed through a decoder neural network, which maps from the latent space to the input space in order to generate data points.\n",
        "4.   **Reconstruction**: The output of the decoder is compared to the original input data, and the difference between them is used to calculate a reconstruction loss. This loss represents how well the VAE is able to reconstruct the input data from the compressed representation in the latent space. The error is backpropagated and minimized using gradient descent techniques.\n",
        "\n",
        "While simple autoencoders are deterministics, the variational autoencoders are probabilistics. The reason for this is, that while the simple ones encode an input as a single point, the variational ones use a distribution over the latent space. This ensures that the representations in the latent space are regularized both locally and globally.\n",
        "\n",
        "In order to improve the performance of the encoding-decoding phase, a reconstruction term is added on the last layer, and a regularisation term (based on the Kullback-Leiber divergence) is added to the latent layer to regularise the organization of the latent space by making the distributions returned by the encoder close to a standard normal distribution. \n",
        "\n",
        "As backpropagation cannot be done for a sampling process of an arbitrary random distribution, a reparametrization trick is applied. This involves sampling auxiliary noise from a standard normal distribution and transforming it using the mean and covariance matrix of the variational distribution to produce a sample from the variational distribution.\n",
        "\n",
        "VAEs have several applications as generative models. They can be used for data generation and augmentation. They can also be used for reconstructing noisy images, but the results would be worse than using simple AEs because points in the latent space are more concentrated, and the effect of the same distorsion is going to be much more noticeable. There are other more creative applications, like generating images from noise, where VAEs would outperform AEs, or making smooth transitions between two images by using interpolations.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BDqyLMCcLKQW"
      },
      "source": [
        "#  Generative Adversarial Networks (GANs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-T0NXD6uLfen"
      },
      "source": [
        "Generative Adversarial Networks (GANs) are also generative and graphical models, but this ones include the concept of discriminative models, which tries to diferenciate two samples. GANs are generative models, so they try to generate samples that are similar to a training set, but for doing so they use two networks:\n",
        "\n",
        "1.   **Generator network**: responsible for generating synthetic data samples. \n",
        "2.   **Discriminator network**: tasked to distinguish the synthetic samples from real samples drawn from the training dataset.\n",
        "\n",
        "Those two networks are training together in an adversarial process: the generator tries to produce samples that the discriminator cannot distinguish from real samples, while the discriminator tries to correctly classify the synthetic and real samples.\n",
        "\n",
        "The discriminator is a simply classifier, so it could use any network architecture appropiate to the type of data it's classifying. For its training it uses data from two sources: real data instances and fake data instances created  by the generator.\n",
        "\n",
        "The generator learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real. While the discriminator has more freedom on its architecture, the generator requires tighter integration with the discriminator. For training it, it is required a random input, a generator network (for transforming the random input into a data instance), a discriminator network (which classifies the generated data), the discriminator output and the generator loss (which penalizes the generator for failing to fool the discriminator). But an extra network is required for backpropagation, because the generator is not direcly connected to the loss that we are trying to affect. \n",
        "\n",
        "\n",
        "After adding this network, the generator is trained with the following procedure:\n",
        "\n",
        "\n",
        "1.   Sample random noise\n",
        "2.   Produce generator output from sampled random noise\n",
        "3.   Get discriminator classification \n",
        "4.   Calculate loss from discriminator classification\n",
        "5.   Backpropagate through both the discriminator and generator to obtain gradients\n",
        "6.   Use gradients to change only the generator weights\n",
        "\n",
        "GANs contain two separately trained networks, so for training it as a whole it is required to train each one for low epochs on each cycle of the GAN trainning. This special kind of training lead to multiple complications related to its convergence. For doing so, several modifications have been proposed, but we are going to focus on Wasserstein GAN with Gradient Penalty (WGAN-GP) and Deep Convolutional GANs (DCGANs).\n",
        "\n",
        "\n",
        "\n",
        "Regarding the differences between GANs and VAEs, the main ones are:\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AtToTWclBGpB"
      },
      "source": [
        "| **GANs** | **VAEs** |\n",
        "| --- | --- |\n",
        "| **Minimize the divergence between the generated distribution <br> and an unknown target distribution in an actor-critic fashion** | **Minimize a bound on the divergence between the generated <br> distribution and a prespecified target distribution** |\n",
        "| **Noisy, difficult and notoriously unstable optimization** | **Faster, reliable and theoretically justified optimization** |\n",
        "| **Only require the “black box” ability to sample from a prior** | **Needs know the prior form as “white box”** |\n",
        "| **Produce ”sharper“ results** | **Produce often blurry results** |\n",
        "| **Learn only a decoder** | **Learn an encoder-decoder pair** |\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8GGhejapLS_u"
      },
      "source": [
        "## Wasserstein GAN with Gradient Penalty (WGAN-GP)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJPmBYt-_Br"
      },
      "source": [
        "The Wasserstein GAN (WGAN) is a type of GAN that includes the Wasserstein loss in order to prevent vanishing gradients and improve the sample quality of the GAN. The Wasserstein loss measures the distance between two probability distributions (real and generated data). The generator minimizes the Wasserstein loss while the discriminator maximizes it.\n",
        "\n",
        "In addition to the Wasserstein loss, WGAN-GP also includes a gradient norm penalty to achieve Lipschitz continuity. This penalty improves stability by penalizing gradients with large norm values at the cost of longer computational time. The combination of the Wasserstein loss and the gradient norm penalty helps to stabilize and improve the sample quality of the GAN.\n",
        "\n",
        "It is important to resemble that the GAN loss measures how well the generator fools the discriminator rather than a measure of the image quality, and it could remain the same while the quality improves. On the other hand, the WGAN loss reflects the image quality, which is more desirable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "je-vhpB3LXrM"
      },
      "source": [
        "## Deep Convolutional GANs (DCGANs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SZkFwT0FLdqa"
      },
      "source": [
        "DCGANs are a type of GAN that explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively. DCGANs have certain architectural constraints and are a strong candidate for unsupervised learning.\n",
        "\n",
        "The use of convolutional and convolutional-transpose layers in the discriminator and generator allows DCGANs to learn hierarchical representations of the data. This helps to improve the sample quality of the GAN and allows it to generate more realistic data.\n",
        "\n",
        "\n",
        "The provided codes for WGAN-GP and DCGAN have several key differences, which we assume are representative of the general differences in their common uses, and are the following ones:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tjM1q79q9Wn4"
      },
      "source": [
        "| WGAN-GP | DCGAN |\n",
        "| --- | --- |\n",
        "| **Uses Wasserstein loss with gradient penalty for the critic and Wasserstein loss for the generator** | **Uses binary cross-entropy loss for both discriminator and generator** |\n",
        "| **Separate training loop for critic and generator, which helps maintain a balance** | **Trains the discriminator and generator simultaneously in a single training loop** |\n",
        "| **Tracks critic loss, critic Wasserstein loss, critic gradient penalty and generator loss** | **Tracks discriminator loss, real and fake accuracy for the discriminator, overall discriminator accuracy, generator loss and generator accuracy** |\n",
        "| **Does not use sigmoid activation in the last layer of the critic, as it calculates Wasserstein loss directly** | **Uses sigmoid activation in the last layer of the discriminator to output probabilities** |\n",
        "| **Doesn't use label smoothing** | **Uses label smoothing to make the training process more robust** |\n",
        "| **Typically uses the RMSProp or Adam optimizers with a lower learning rate than DCGAN** | **Often uses Adam with higher learning rate** |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
